= Workshop: Building Event-driven microservices using Spring Boot, Kafka Streams and ksqlDB
Viktor Gamov <viktor@confluent.io>, © 2020 Confluent, Inc.
2020-08-25
:revdate: 2020-08-25 19:22:48 -0600
:linkattrs:
:ast: &ast;
:y: &#10003;
:n: &#10008;
:y: icon:check-sign[role="green"]
:n: icon:check-minus[role="red"]
:c: icon:file-text-alt[role="blue"]
:toc: auto
:toc-placement: auto
:toc-position: auto
:toc-title: Table of contents
:toclevels: 3
:idprefix:
:idseparator: -
:sectanchors:
:icons: font
:source-highlighter: highlight.js
:highlightjs-theme: idea
:experimental:
:imagesdir: ../images
ifndef::awestruct[]
:awestruct-draft: false
:awestruct-layout: post
:awestruct-tags: []
:idprefix:
:idseparator: -
endif::awestruct[]


Developing Event-driven Microservices with Spring Boot, Confluent Cloud, Kotlin, and Java.

toc::[]

== Workshop pre-requisites and preparation

Before you proceed, make sure you have the following toolset installed on your computer:

* Sign up for https://confluent.cloud[Confluent Cloud]
** you should have your login and password handy after you sign up for Confluent Cloud.
+
The `ccloud` init script will ask login.
** Install https://docs.confluent.io/current/cloud/cli/install.html[Confluent Cloud CLI]
* Docker
** https://docs.docker.com/docker-for-mac/install/[MacOS]
** Docker Compose (installed with Docker Desktop and Docker Toolbox)
* git
* Java
** https://jdk.dev[Java 8 (or later)]
* Your favorite Java IDE  
** I recommend https://www.jetbrains.com/idea/[IntelliJ IDEA]
* As another prerequisite, clone following repo 
+

[source,shell script]
----
$ git clone https://github.com/gAmUssA/springone2020-workshop    #<1>
$ cd springone2020-workshop                                      #<2>
----
<1> Clone the repository
<2> Change directory of the workshop folder

== 0️⃣ Provisioning Confluent Cloud cluster

[source,shell script]
----
$ cd scripts/ccloud
$ ccloud login --save       #<1>
$ ./ccloud_stack_create.sh  #<2>

This demo uses real Confluent Cloud resources.
To avoid unexpected charges, carefully evaluate the cost of resources before launching the script and ensure all resources are destroyed after you are done running it.
Do you still want to run this script? [y/n] y
Do you also want to create a Confluent Cloud ksqlDB app (hourly charges may apply)? [y/n] y

Creating Confluent Cloud stack for service account demo-app-3067, ID: 103469.
Set Kafka cluster "lkc-oz98y" as the active cluster for environment "env-nx57d".

Waiting up to 720 seconds for Confluent Cloud cluster to be ready and for credentials to propagate
.....
Sleeping an additional 80 seconds to ensure propagation of all metadata

$ source delta_configs/env.delta  #<3>
----
<1> Login to your Confluent Cloud account.
<2> The CCloud Stack script will ask you to login to your CCloud account.
It will automatically provision Kafka and ksqlDB cluster.
<3> export Kafka connection information as environment variables.

Among other things, this script generates a config that we need to pass to docker-compose start command in order to connector container connect to cloud Kafka cluster. 

When ready, move to the next section where you will generate some referential data.

== 1️⃣ Loading referential data with Kafka Connect

To leverage the full power of stream processing, it is a good practice to pre-load the required data in topics.
Kafka Streams and ksqlDB will allow you to join and lookup data from your events with any other topic.

In this section of the workshop, we will set up a https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc[Kafka Connect JDBC Source connector] instance that will synchronize any data from a PostgreSQL instance to an `account` topic in Kafka.

This exercise simulates a https://en.wikipedia.org/wiki/Change_data_capture[Change Data Capture pattern] where we bridge an existing data source to Kafka in realtime.

image::jdbc-source-connector.png[JDBC Source Connector]

=== Start JDBC connector in Docker

[source,shell script]
----
./start.sh stack-configs/java-service-account-103523.config #<1>
----
<1> Replace with actual service account ID

==== Start the Data Generator application

Within the workshop project, you will find a `data-generator` folder containing an application designed to generate some random accounts in our PostgreSQL `Account DB`.
This utility application will generate around `1000` test accounts.
The Data Generator also contains a REST Endpoint to help us submit transaction requests to Kafka later on during the workshop.

The data generator can be launched by running the following command from the root of the workshop project folder:

image::data-generator.png[Data Generator]

To run the Data Generator application, you can use your Java IDE to launch the main method from `src/main/java/io/confluent/developer/ccloud/demo/kstream/DataGeneratorApplication.java`.

You can also start the application from CLI by building it and running it:

[source,shell script]
----
$ source ./scripts/cclou/delta_configs/env.delta
$ cd ../..
$ ./gradlew :data-generator:build                        #<1>
$ java -jar data-generator/build/libs/data-generator-0.0.1-SNAPSHOT.jar      #<2>
----
<1> To build
<2> To run after build

After the data set completely generated, you should observe an output mentioning that 1000 accounts created:

----
2020-08-26 22:58:44.507  INFO 15959 --- [unt-Generator-1] Account Service                          : Generated account number 1000.
----

=== Start the connector

Run the following command from the root of the workshop project folder:

[source,shell script]
----
$ ./scrips/connect/deploy-jdbc-connector.sh #<1>
----
<1> This command will start a connector instance.

=== Monitor the account data flowing in Kafka from Confluent Cloud UI

Access Confluent Cloud UI from https://confluent.cloud

From the main screen, navigate to environment that looks like `demo-env-<some-number>`.
Inside of this environment you should see cluster that looks like `demo-kafka-cluster-<some-number>`.
On the left side click on `Topics`.

Click on the `account` topic and access the `messages` tab.
Click on the `offset` textbox and type `0` and press enter to the UI to load all messages from partition `0` starting from __offset__ `0`.

With the connector running, you should observe `account` events in the UI.

.Messages explorer in Confluent Cloud UI
image::cloud-ui-messages.jpg[c3-messages]

In the next section, we will implement a highly scalable stream processing application using Kafka Stream.

== 2️⃣ Implementing a Stream Processor with Kafka Streams

Now is the time to get into the heart of the action.
We will implement a Kafka Streams topology to process atomic transactions to any request submitted to the `transaction-request` topic.

Within the workshop project folder, you will find a `kstreams-demo` subfolder that represents a Kafka Streams application.
All of the boilerplate code required to connect to Kafka is already taken care of (thank you, Spring Boot).
This workshop will focus on writing a Kafka Streams topology with the function processing for our use case.

[WARNING]
."Help me! I can't figure out what code to modify!"
====
If during the exercise you are lost, you can at any point reset your codebase and switch to the `solution` to run the Stream Processor without coding the solution yourself.

Be careful before running the next command as you will lose any uncommitted changes in your local git repository:

----
git reset --hard origin/master && git checkout solution
----
====

=== Atomic transaction processing with Kafka Streams

Our business requirement states that for every request we receive, we check if the funds are sufficient before updating the balance of the account being processed.
We should never have two transactions being processed at the same time for the same account.
This would create a race condition for which we have no guarantee that we can enforce the balance check before withdrawing funds.

The Data Generator writes transaction requests to the Kafka topic with a key equals to the account number of the transaction.
As such, we have the guarantee that all messages of an account will be proccessed by a single thread for our Transaction Service no matter how many instances of it are concurrently running.

Kafka Streams will not commit any message offset until it completes our business logic of managing a transaction request.

image::transaction-service.png[Transaction Service]

==== Implement the Transaction Transformer

Because of the transaction nature of our stream processor, we require a specific component from Kafka Streams named a `Transformer`. 
This utility allows us to process events one by one while interacting with a `State Store`, another component of Kafka Streams that allows us to persist our account balance.

Open the `io.confluent.developer.ccloud.demo.kstream.TransactionTransformer` Java class and implement the `transform` function to return a `TransactionResult` based on the validity of the transaction request.
The `TransactionResult` contains a `success` flag that should be set to `true` if the funds were successfully updated.

The `transform` method also has the responsibility of updating the `store` State Store.
The class already contains utility functions to help you execute our business logic.

If you are stuck on this exercise, you can switch to the `solution-transformer` branch:

----
git reset --hard origin/master && git checkout solution-transformer
----

==== Implement the Streaming Topology

In Kafka Streams, a `Topology` is the definition of your data flow.
It's a manifest for all operations and transformations to be applied to your data.

To start a stream processor, Kafka Streams only requires us to build a `Topology` and to hand it over.
Kafka Streams will take care of managing the underlying consumers and producers.

The `io.confluent.developer.ccloud.demo.kstream.KStreamConfig` Java class already contains all the boilerplate code required by Kafka Streams to start our processor.
In this exercise, we will leverage a `StreamsBuilder` to define and instantiate a `Topology` that will handle our transaction processing.

Open the `io.confluent.developer.ccloud.demo.kstream.KStreamConfig.defineStreams` method and get ready to write your first Kafka Streams Topology.

==== Create a KStream from the source topic.

Use the `stream` method of `streamsBuilder` to turn a topic into a `KStream`.

[source,java]
----
KStream<String, Transaction> transactionStream = 
  streamsBuilder.stream("transaction-request");
----

==== Leverage the Transformer to process our requests

To inform Kafka Streams that we want to update the `funds` State Store for all incoming requests atomically, we can leverage the `transformValues` operator to plugin our `TransactionTransformer`. 
This operator requires us to specify the name of the `funds` State Store that will be used by the `Transformer`. 
This also instructs Kafka Streams to keep track of events from our `transaction-request` since they will result in a change of state for our store.

[source,java]
----
KStream<String, TransactionResult> resultStream = transactionStream
  .transformValues(this::transactionTransformer, "funds");
----

==== Redirect the transaction result to the appropriate topic.

With a new derived stream containing `TransactionResult`, we can now use the information contained in the payload to feed a success or failure topic.

We will achieve by deriving two streams from our `resultStream`. 
Each will be built by applying a `filter` and `filterNot` operator with a predicate on the `success` flag from our `TransactionResult` payload.
With the two derived streams, we can explicitly call the `to` operator to instruct Kafka Streams to write the mutated events to their respective topics.

[source,java]
----
resultStream
  .filter(this::success)
  .to("transaction-successs");

resultStream
  .filterNot(this::success)
  .to("transaction-failed");
----

==== The implemented defineStreams method

Use this reference implementation to validate that you have the right stream definition.

[source,java]
----
private void defineStreams(StreamsBuilder streamsBuilder) {
  KStream<String, Transaction> transactionStream = 
    streamsBuilder.stream("transaction-request");


  KStream<String, TransactionResult> resultStream = transactionStream
    .transformValues(
      this::transactionTransformer, "funds"
    );

  resultStream
    .filter(this::success)
    .to("transaction-successs");

  resultStream
    .filterNot(this::success)
    .to("transaction-failed");
  }
----

=== Running the Kafka Streams application

If you are running the application from your Java IDE.
Just launch the main method from `io.confluent.developer.ccloud.demo.kstream.KStreamDemoApplication`.

If you want to run with the CLI, you will need to build the application before launching it.

To build:

----
./mvnw -f kstreams-demo/pom.xml clean package
----

To run:

----
java -jar kstreams-demo/target/kstreams-demo.jar
----

=== Generate some transactions using the Data Generator endpoint

Ensure that you Data Generator application is still running from link:../connector/connector-linux.md#start-the-data-generator-application[the previous section].

The utility script `scripts/generate-transaction.sh` will let you generate transactions.
Generate a couple of transaction with the following commands:

----
scripts/generate-transaction.sh 1 DEPOSIT 100 CAD
scripts/generate-transaction.sh 1 DEPOSIT 200 CAD
scripts/generate-transaction.sh 1 DEPOSIT 300 CAD
scripts/generate-transaction.sh 1 WITHDRAW 300 CAD
scripts/generate-transaction.sh 1 WITHDRAW 10000 CAD

scripts/generate-transaction.sh 2 DEPOSIT 100 CAD
scripts/generate-transaction.sh 2 DEPOSIT 50 CAD
scripts/generate-transaction.sh 2 DEPOSIT 300 CAD
scripts/generate-transaction.sh 2 WITHDRAW 300 CAD
----

The script takes in argument the account number, the amount, the type of operation (`DEPOSIT` or `WITHDRAW`) and the currency.

=== Monitor the successful transaction results 

Access Confluent Cloud UI from https://confluent.cloud

From the main screen, navigate to environment that looks like `demo-env-<some-number>`.
Inside of this environment you should see cluster that looks like `demo-kafka-cluster-<some-number>`.
On the left side click on `Topics`.

Click on the `transaction-success` topic and access the `messages` tab.
Click on the `offset` textbox and type `0` and press enter to load all messages from partition 0 starting from offset 0.

You should observe `transaction-success` events in the UI.
If you can't see any message, try your lock with partition 1 starting from offset 0.

// TODO
//image::transaction-success.png[transaction-success]

=== Monitor the failed transaction results from Control Center

Click on the `topic` tab from the cluster navigation menu.
Select the `transaction-failed` topic and access the `messages` tab.
Click on the `offset` textbox and type `0` and press enter to load all messages from partition 0 starting from offset 0.

You should observe `transaction-failed` events in the UI.
If you can't see any message, try your lock with partition 1 starting from offset 0.

// TODO
// image::transaction-failed.png[transaction-failed]

In the next section, we will explore how writing Stream Processor can even be more simplified with `ksqlDB`.

== 3️⃣ Enrich transaction results with ksqlDB

If you recall, in the first section of this workshop, we configured a JDBC Source Connector to load all account details into an `account` topic.
In this next exercise, we will write a second Stream Processor to generate a detailed transaction statement enriched with account details.

Rather than within this new service as another Kafka Streams application, we will leverage ksqlDB to declare a stream processor who will enrich our transaction data in real-time with our referential data coming from the `account` topic.
The objective of this section is to showcase how an SQL-like query language be used to generate streams processors just like Kafka Streams without having to compile and run any custom piece of software.

image::transaction-statement-overview.png[Transaction Statements]

.Connect to ksqlDB with CLI
****
In this excersise, we're going to use ksqlDB Cloud UI.
But you also can run cli using docker.

----
docker run -it confluentinc/ksqldb-cli:0.11.0 ksql -u $KSQL_API_KEY -p $KSQL_API_SECRET $KSQLDB_ENDPOINT
----

****

=== Create the account table

// ksql config

// ccloud ksql app list
// ccloud kafka cluster list
// ccloud ksql app configure-acls lksqlc-7k6dj account --cluster lkc-nro63

ksqlDB is built on top of Kafka Streams.
As such, the `KStream` and `KTable` are both key constructs for defining stream processors.

The first step requires us to instruct ksqlDB that we wish to turn the `account` topic into a `Table`. This table will allow us to join each `transaction-success` event with the latest `account` event of the underlying topic.
Run the following command in your ksqlDB CLI terminal:

[source,sql]
----
CREATE TABLE ACCOUNT (
  numkey string PRIMARY KEY,
  number INT,
  cityAddress STRING,
  countryAddress STRING,
  creationDate BIGINT,
  firstName STRING,
  lastName STRING,
  numberAddress STRING,
  streetAddress STRING,
  updateDate BIGINT
) WITH (
  KAFKA_TOPIC = 'account',
  VALUE_FORMAT='JSON'
);
----

=== Create the transaction-success stream

Before we create the `Transaction Statement` stream processor, we also need to inform ksqlDB that we wish to turn the `transaction-success` into a `Stream`. 
Run the following command in your ksqlDB CLI terminal:

[source,sql]
----
CREATE STREAM TRANSACTION_SUCCESS (
  transaction STRUCT<guid STRING, account STRING, amount DOUBLE, type STRING, currency STRING, country STRING>,
  funds STRUCT<account STRING, balance DOUBLE>,
  success boolean,
  errorType STRING
) WITH (
  kafka_topic='transaction-success',
  value_format='json'
);
----

=== Create the transaction statement stream

Now that we have all the ingredients of our `Transaction Statement` stream processor, we can now create a new stream derived from our `transaction-success` events paired with the latest data from the `account` topic.
We will instruct ksqlDB to create a new stream as a Query.
By default, ksqlDB will publish any output to a new `TRANSACTION_STATEMENT` topic.
The select query provides the details about with events to subscribe as well as which table to join each notification.
The output of this new stream processor will be a mix of the transaction details coupled with all the details of the matching account.
The key from `transaction-success` and `account` will be used as matching criteria for the `LEFT JOIN` command. `EMIT CHANGES` informs ksqlDB that this query is long-running and should continuously be kept alive.
Just as if it was a Kafka Streams application to be 100% available to process all events.
Run the following command in your ksqlDB CLI terminal:

[source,sql]
----
CREATE STREAM TRANSACTION_STATEMENT AS
  SELECT *
  FROM TRANSACTION_SUCCESS
  LEFT JOIN ACCOUNT ON TRANSACTION_SUCCESS.rowkey = ACCOUNT.rowkey
  EMIT CHANGES;
----

=== Monitor the Transaction Statements in Cloud UI

Access Confluent Cloud UI from https://confluent.cloud

From the main screen, navigate to environment that looks like `demo-env-<some-number>`.
Inside of this environment you should see cluster that looks like `demo-kafka-cluster-<some-number>`.
On the left side click on `Topics`.

Click on the `TRANSACTION_STATEMENT` topic and access the `messages` tab.
Click on the `offset` textbox and type `0` and press enter to instruct C3 to load all messages from partition 0 starting from offset 0.

image::transaction-statements.png[c3-transaction-statements]

== ✅ It's a wrap!

Congratulations! Now you know how to build event-driven microservices using Spring Boot, Kafka Streams and ksqlDB.

.Don't forget to clean up
****

[source,shell script]
----
$ cd scripts/ccloud
$ docker-compose down -v    #<1>
$ ./ccloud_stack_destroy.sh  stack-configs/java-service-account-103523.config #<2>
----
<1> Stop a connector and database
<2> Destroy ccloud stack to avoid unexpected charges.

****

== Special Thanks! 

This workshop based on work of https://github.com/daniellavoie[Daniel Lavoie].
Much ♥️!
